{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medinavi/2DTS/blob/main/RL_Exerc%C3%ADcio_1_Q_Learning_Cliff_Walking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neste notebook, voc√™ codificar√° do zero seu segundo agente de Reinforcement Learning jogando Cliff Walking usando Q-Learning"
      ],
      "metadata": {
        "id": "clnLGkknvrG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptado HuggingFace"
      ],
      "metadata": {
        "id": "tQFWPAzi0tbk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://www.gymlibrary.dev/_images/cliff_walking.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üéÆ Environments:\n",
        "\n",
        "> \n",
        "\n",
        "- [CliffWalking-v0](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)\n",
        "\n",
        "\n",
        "###üìö RL-Library: \n",
        "\n",
        "- Python and NumPy\n",
        "- [Gym](https://www.gymlibrary.dev/)"
      ],
      "metadata": {
        "id": "DPTBOv9HYLZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar depend√™ncias e criar um display virtual üîΩ\n"
      ],
      "metadata": {
        "id": "4gpxC1_kqUYe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.24\n",
        "!pip install pygame\n",
        "!pip install numpy\n",
        "\n",
        "!pip install huggingface_hub\n",
        "!pip install pickle5\n",
        "!pip install pyyaml==6.0\n",
        "!pip install imageio\n",
        "!pip install imageio_ffmpeg\n",
        "!pip install pyglet==1.5.1\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get update\n",
        "!apt install python-opengl ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "n71uTX7qqzz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para garantir que as novas bibliotecas instaladas sejam usadas, **√†s vezes √© necess√°rio reiniciar o tempo de execu√ß√£o do notebook**. A pr√≥xima c√©lula for√ßar√° o **tempo de execu√ß√£o a travar, ent√£o voc√™ precisar√° se conectar novamente e executar o c√≥digo a partir daqui**."
      ],
      "metadata": {
        "id": "K6XC13pTfFiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Importa√ß√£o de pacotes üì¶\n",
        "\n",
        "Al√©m das bibliotecas instaladas, utilizamos tamb√©m:\n",
        "\n",
        "- `random`: Para gerar n√∫meros aleat√≥rios (que ser√£o √∫teis para a pol√≠tica epsilon-greedy).\n",
        "- `imageio`: Para gerar um v√≠deo de replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Part 1: CliffWalking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## Criando o ambiente CliffWalking-v0 (https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)\n",
        "---\n",
        "\n",
        "üí° Um bom h√°bito quando voc√™ come√ßa a usar um ambiente √© verificar sua documenta√ß√£o\n",
        "\n",
        "üëâ https://www.gymlibrary.dev/environments/toy_text/cliff_walking/\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "#env = gym.make(\"CliffWalking-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### Verifique o Environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "576CjY4M7Dsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "ZKZD-1W37jJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Definindo os hiperpar√¢metros ‚öôÔ∏è\n",
        "Os hiperpar√¢metros relacionados √† explora√ß√£o s√£o alguns dos mais importantes.\n",
        "\n",
        "- Precisamos garantir que nosso agente **explore o espa√ßo de estados** o suficiente para aprender uma boa aproxima√ß√£o de valor. Para fazer isso, precisamos ter decaimento progressivo do epsilon.\n",
        "- Se voc√™ diminuir o epsilon muito r√°pido (decay_rate muito alto), **voc√™ corre o risco de que seu agente fique preso**, j√° que seu agente n√£o explorou o espa√ßo de estado o suficiente e, portanto, n√£o pode resolver o problema."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "id": "T0LRfPIg8GZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos criar nossa Qtable de tamanho (state_space, action_space) e inicializar cada valor em 0 usando np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "K9HwoJdn8RW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_CliffWalking = initialize_q_table(state_space, action_space)"
      ],
      "metadata": {
        "id": "JFEH1r6C8T5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_CliffWalking"
      ],
      "metadata": {
        "id": "y4JAkBuE8WMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "  \n",
        "  return action"
      ],
      "metadata": {
        "id": "qkiUgFlm8bP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Gera aleatoriamente um n√∫mero entre 0 e 1\n",
        "  random_int = random.uniform(0,1)\n",
        "  # if random_int > maior que epsilon --> exploitation\n",
        "  if random_int > epsilon:\n",
        "     # Execute a a√ß√£o com o maior valor dado um estado\n",
        "     # np.argmax pode ser √∫til aqui\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "  \n",
        "  return action"
      ],
      "metadata": {
        "id": "KyMY6Czc8e_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Par√¢metros de treinamento\n",
        "n_training_episodes = 10000 # Total de epis√≥dios de treinamento\n",
        "learning_rate = 0.7 # Taxa de aprendizado\n",
        "\n",
        "# Par√¢metros de avalia√ß√£o\n",
        "n_eval_episodes = 100 # N√∫mero total de epis√≥dios de teste\n",
        "\n",
        "# Par√¢metros do ambiente\n",
        "env_id = \"CliffWalking-v0\" # Nome do ambiente\n",
        "max_steps = 99 # Max passos por epis√≥dio\n",
        "gamma = 0.95 # Taxa de desconto\n",
        "eval_seed = [] # A semente de avalia√ß√£o do ambiente\n",
        "\n",
        "# Par√¢metros de explora√ß√£o\n",
        "max_epsilon = 1.0 # Probabilidade de explora√ß√£o no in√≠cio\n",
        "min_epsilon = 0.05 # Probabilidade m√≠nima de explora√ß√£o\n",
        "decay_rate = 0.0005 # Taxa de decaimento exponencial para prob de explora√ß√£o"
      ],
      "metadata": {
        "id": "sZ_nEis28k5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # # Reduzir epsilon (porque precisamos cada vez menos exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Redefinir o ambiente\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repete\n",
        "    for step in range(max_steps):\n",
        "      # Escolha a a√ß√£o At para usar a pol√≠tica gananciosa (greedy policy) do epsilon \n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   \n",
        "\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "        break\n",
        "      \n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "IpPCKiuy8urT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Treinando o agente Q-Learning üèÉ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_CliffWalking = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_CliffWalking)"
      ],
      "metadata": {
        "id": "HX7QAd5n85RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Verificando a tabela Q-Learning üëÄ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_CliffWalking"
      ],
      "metadata": {
        "id": "aSd0Pu6J-Dz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## Avalia√ß√£o do M√©todo üìù\n",
        "\n",
        "- Definimos o m√©todo de avalia√ß√£o que vamos usar para testar nosso agente Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "   Avalie o agente para epis√≥dios ``n_eval_episodes`` e retorne recompensa m√©dia e padr√£o de recompensa.\n",
        "   :param env: O ambiente de avalia√ß√£o\n",
        "   :param n_eval_episodes: N√∫mero de epis√≥dios para avaliar o agente\n",
        "   :param Q: A tabela Q\n",
        "   :param seed: A matriz de sementes de avalia√ß√£o (para taxi-v3)\n",
        "   \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "7lCc3me--LS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Avaliando nosso agente Q-Learning üìà\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_CliffWalking, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "qt9JkzZ8-RpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### N√£o modifique essa parte\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def play_actions(env, Qtable, delay = 1):\n",
        "  \"\"\"\n",
        "   Gerar um v√≠deo de replay do agente\n",
        "   :param env\n",
        "   :param Qtable: Qtable do nosso agente\n",
        "   :param out_directory\n",
        "   :param fps: quantos quadros por segundo (com taxi-v3 e frozenlake-v1 usamos 1)\n",
        "   \"\"\"\n",
        "\n",
        "  sequenses = []  \n",
        "  done = False\n",
        "  state = env.reset(seed=random.randint(0,500))\n",
        "  txt = env.render(mode='human')\n",
        "\n",
        "  sequenses.append(txt)\n",
        "  while not done:\n",
        "    # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
        "    \n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    print(action)\n",
        "    state, reward, done, info = env.step(action) # Colocamos diretamente next_state = state para a l√≥gica de grava√ß√£o\n",
        "    txt = env.render(mode='human')\n",
        "    sequenses.append(txt)\n",
        "    print(txt)\n",
        "    time.sleep(delay)\n",
        "  \n",
        "  return sequenses\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_actions(env, Qtable_CliffWalking,  1)"
      ],
      "metadata": {
        "id": "PKrxycyWylq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "   Gerar um v√≠deo de replay do agente\n",
        "   :param env\n",
        "   :param Qtable: Qtable do nosso agente\n",
        "   :param out_directory\n",
        "   :param fps: quantos quadros por segundo (com taxi-v3 e frozenlake-v1 usamos 1)\n",
        "   \"\"\"\n",
        "  images = []  \n",
        "  done = False\n",
        "  state = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Tome a a√ß√£o (√≠ndice) que tem a recompensa futura m√°xima esperada dado aquele estado\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, done, info = env.step(action) # Colocamos diretamente next_state = state para a l√≥gica de grava√ß√£o\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "metadata": {
        "id": "XmfmHWD5-dWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0q-4EEc_Hf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}